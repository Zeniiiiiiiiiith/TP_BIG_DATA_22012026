services:
  # -----------------------
  # Object Storage (Lakehouse)
  # -----------------------
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_REGION: ${MINIO_REGION}
    volumes:
      - minio-data:/data
    networks: [platform]

  minio-init:
    image: minio/mc:latest
    depends_on: [minio]
    entrypoint: ["sh", "/init-minio.sh"]
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - ./init/minio/init-minio.sh:/init-minio.sh:ro
    networks: [platform]

  # -----------------------
  # Kafka
  # -----------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks: [platform]

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    depends_on: [zookeeper]
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks: [platform]

  kafka-init:
    image: confluentinc/cp-kafka:7.6.1
    depends_on: [kafka]
    entrypoint: ["/bin/sh", "/init-kafka.sh"]
    volumes:
      - ./init/kafka/init-kafka.sh:/init-kafka.sh:ro
    networks: [platform]

  # -----------------------
  # Filebeat (simule "agents" logs -> Kafka)
  # -----------------------
  filebeat:
    image: elastic/filebeat:8.14.3
    user: root
    depends_on: [kafka]
    command: ["-e", "--strict.perms=false"]
    volumes:
      - ./init/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./data/logs:/var/log/custom:ro
    networks: [platform]

  # -----------------------
  # Spark (Streaming + Batch)
  # -----------------------
  spark-master:
    image: apache/spark:3.5.1-scala2.12-java11-python3-ubuntu
    command: ["/bin/bash", "-lc", "/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"]
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./init/spark/jobs:/opt/spark/jobs
      - ./init/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    networks: [platform]

  spark-worker:
    image: apache/spark:3.5.1-scala2.12-java11-python3-ubuntu
    depends_on: [spark-master]
    command: ["/bin/bash", "-lc", "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /opt/spark/logs/*"]
    ports:
      - "8081:8081"
    volumes:
      - ./init/spark/jobs:/opt/spark/jobs
      - ./init/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    networks: [platform]

  # -----------------------
  # Airflow (Batch Orchestration)
  # -----------------------
  airflow-db:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db:/var/lib/postgresql/data
    networks: [platform]

  airflow-init:
    image: apache/airflow:2.9.3
    depends_on: [airflow-db]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    entrypoint: /bin/bash
    command: -lc "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"
    networks: [platform]

  airflow:
    image: apache/airflow:2.9.3
    depends_on: [airflow-db, airflow-init, minio]
    command: ["bash", "-lc", "airflow webserver"]
    ports:
      - "8088:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./init/airflow/dags:/opt/airflow/dags
      - ./data/csv:/data/csv:ro
    networks: [platform]

  # -----------------------
  # Iceberg
  # -----------------------

  iceberg-catalog-db:
    image: postgres:16
    environment:
      POSTGRES_USER: iceberg
      POSTGRES_PASSWORD: iceberg
      POSTGRES_DB: iceberg
    ports:
      - "5433:5432"
    networks: [platform]

  # -----------------------
  # Trino (SQL sur Gold)
  # -----------------------
  trino:
    image: trinodb/trino:455
    depends_on: [minio, iceberg-catalog-db]
    ports:
      - "8082:8080"
    volumes:
      - ./init/trino/iceberg.properties:/etc/trino/catalog/iceberg.properties:ro
    networks: [platform]

  # -----------------------
  # MLflow
  # -----------------------
  mlflow-db:
    image: postgres:16
    profiles: ["ml"]
    environment:
      POSTGRES_USER: ${MLFLOW_USER}
      POSTGRES_PASSWORD: ${MLFLOW_PASSWORD}
      POSTGRES_DB: ${MLFLOW_DB}
    volumes:
      - mlflow-db:/var/lib/postgresql/data
      - ./init/mlflow/init-mlflow.sql:/docker-entrypoint-initdb.d/init-mlflow.sql:ro
    networks: [platform]

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.3
    profiles: ["ml"]
    depends_on: [mlflow-db, minio]
    ports:
      - "5000:5000"
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri ${MLFLOW_BACKEND_STORE}
      --default-artifact-root ${MLFLOW_ARTIFACT_ROOT}
    networks: [platform]

  # -----------------------
  # Feast (Feature Store)
  # -----------------------
  feast-redis:
    image: redis:7
    profiles: ["ml"]
    ports:
      - "6379:6379"
    networks: [platform]

  # -----------------------
  # Scoring service (Kafka consumer + API)
  # -----------------------
  scoring:
    build:
      context: ./init/scoring
      dockerfile: Dockerfile
    profiles: ["ml"]
    depends_on: [kafka]
    ports:
      - "8000:8000"
    environment:
      KAFKA_BOOTSTRsT: kafka:9092
      KAFKA_TOPIC_IN: parsed-events
      KAFKA_TOPIC_OUT: alerts
    networks: [platform]

  # -----------------------
  # Observability (Prometheus + Grafana)
  # -----------------------
  prometheus:
    image: prom/prometheus:v2.54.1
    profiles: ["obs"]
    ports:
      - "9090:9090"
    volumes:
      - ./init/observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    networks: [platform]

  grafana:
    image: grafana/grafana:11.1.4
    profiles: ["obs"]
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    networks: [platform]

networks:
  platform:

volumes:
  minio-data:
  airflow-db:
  mlflow-db:
  grafana-data:
